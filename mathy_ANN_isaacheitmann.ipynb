{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an Artificial Neural Network without ML Libraries \n",
    "\n",
    "$$ \\text{We can rely on the \textbf{\underline{definition of the derivative}} to compute the gradient:}$$\n",
    "\n",
    "$$f(w_0, ..., w_n) = MSE(w_0, ..., w_n)$$\n",
    "\n",
    "$$w_i = w_i -  \\lim_{h\\to 0}\\frac{f(w_i + h, ..., w_n) - f(w_i, ..., w_n)}{h}$$\n",
    "\n",
    "$$\\text{(Figure 1: Partial derivative calculation)}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [0, 0, 1, 1, 0], 1: [1, 0, 0, 1, 0], 2: [0, 1, 0, 0, 1], 3: [0, 0, 1, 1, 1]}\n",
      "{0: 1, 1: 0, 2: 0, 3: 1}\n"
     ]
    }
   ],
   "source": [
    "# toy dataset\n",
    "xdata = {0: [0, 0, 1, 1, 0], 1: [1, 0, 0, 1, 0], 2: [0, 1, 0, 0, 1], 3: [0, 0, 1, 1, 1]}\n",
    "ydata = {0:1, 1:0, 2:0, 3:1}\n",
    "\n",
    "print(xdata)\n",
    "print(ydata)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the network as a class\n",
    "class my_neural_net: \n",
    "    \"\"\"\n",
    "    Artificial neural network, loosely based on the Multilayer Perceptron model. \n",
    "    Attributes: \n",
    "        > depth: number of hidden neural network layers. default 3. \n",
    "        > nodesrow: number of nodes per hidden network layer. default 3. \n",
    "        > num_bias: number of bias terms per hidden network layer. default 1.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, depth=3, nodesrow=3, num_bias=1):\n",
    "        \"\"\"\n",
    "        Create the neural network. \n",
    "        \"\"\"\n",
    "        self.depth = depth\n",
    "        self.nodesrow = nodesrow\n",
    "        self.num_bias = num_bias\n",
    "        \n",
    "    #_____Useful Functions_____\n",
    "    def MSE(y, y_hat): # Mean Squared Error function, use to find loss\n",
    "        \"\"\"\n",
    "        Return mean squared error (MSE) of lists y and y_hat. \n",
    "        \"\"\"\n",
    "        ers = 0\n",
    "        for x in range(len(y)-1):\n",
    "            ers += (y[x]  -  y_hat[x])**2 \n",
    "        return ers / len(y)\n",
    "    \n",
    "    def MAE(y, y_hat): # Mean absolute error\n",
    "        \"\"\"\n",
    "        Return mean absolute error (MAE) of lists y and y_hat. \n",
    "        \"\"\"      \n",
    "        return sum([abs(y[x] - y_hat[x]) for x in range(len(y))]) / len(y)\n",
    "    \n",
    "    def reluL(x): # leaky relu\n",
    "        \"\"\"\n",
    "        Leaky relu (rectified linear unit, but output 0.001 instead of 0 when input < 0). \n",
    "        \"\"\"\n",
    "        return x if x>0 else 0.001\n",
    "    \n",
    "    def relu(x): # Normal relu (rectified linear unit)\n",
    "        \"\"\"\n",
    "        relu (rectified linear unit). \n",
    "        \"\"\"\n",
    "        return x if x>0 else 0\n",
    "       \n",
    "    #_____Fit Model_____    \n",
    "    def modfit(self, xdata, ydata, max_attempts=100, lr=0.1, hnaf=reluL, lossfunc=MSE):\n",
    "\n",
    "        \"\"\"\n",
    "        Generate nodes, then fit the neural network on xdata and ydata. \n",
    "\n",
    "        Parameters: \n",
    "        > xdata: dict or Pandas Series / DataFrame. Features for model to use in predicting y. \n",
    "        > ydata: dict or Pandas Series / DataFrame. Target data which model predicts using xdata. \n",
    "        xdata and ydata should have the same number of rows. \n",
    "        > max_attempts: number of times the network should run + adjust during modfit process. \n",
    "        default 100. \n",
    "        > learning rate: number to multiply each partial derivative (loss/weights) during back propogation. \n",
    "        default 0.1. \n",
    "        > hnaf: Hidden node activation function (ex. relu, reluL, identity). Default leaky relu (reluL). \n",
    "        > lossfunc: loss function (example: Mean Squared Error (MSE)). Default MSE. \n",
    "        \"\"\"\n",
    "        \n",
    "        # store\n",
    "        self.xdata = xdata\n",
    "        self.ydata = ydata\n",
    "        self.max_attempts = max_attempts\n",
    "        self.lr = lr\n",
    "        self.hnaf = hnaf\n",
    "        self.lossfunc = lossfunc\n",
    "\n",
    "        import random #needed\n",
    "        import numpy as np\n",
    "        \n",
    "        # storing data\n",
    "        self.inputs = self.xdata # restructure here \n",
    "\n",
    "        # note on storage and time complexity: \n",
    "        # \n",
    "        #_____Weight Generation_____\n",
    "        def generate_weights(xdata, depth, nodesrow, num_bias):\n",
    "            \"\"\"\n",
    "            Generate the weights & bias terms to be used in network evaluation. \n",
    "            \"\"\"\n",
    "            # NAMING CONVENTION: layer_inputnode_outputnode\n",
    "            weights = {} # to hold weight values\n",
    "            \n",
    "            # input layer -> first hidden layer\n",
    "            for i in range(len(xdata.keys())): # for each input node / feature,\n",
    "                for j in range(nodesrow): # for each output node, \n",
    "                    weights[\"0\" + str(i) + str(j)] =  random.uniform(0,0.5)\n",
    "                \n",
    "            for k in range(num_bias): # bias term(s) \n",
    "                weights[\"0b\" + str(k)] = random.uniform(0,1)\n",
    "            \n",
    "            # move through hidden layers until we reach the output layer\n",
    "            for i in range(depth): # for each layer, \n",
    "                \n",
    "                if bool(depth - i - 1): # if the next layer is also hidden... \n",
    "                    for j in range(nodesrow+num_bias): # for each node in previous layer (including bias), (input node)\n",
    "                        for h in range(nodesrow):# for each node in next layer, (output node)\n",
    "                            weights[str(i+1) +  str(j) + str(h)] = random.uniform(0,0.5)\n",
    "                        \n",
    "                    for k in range(num_bias): # bias term(s) \n",
    "                        weights[str(i+1) + \"b\" + str(k)] = random.uniform(0,1)\n",
    "                    \n",
    "                else: # next layer is the output layer\n",
    "                    for j in range(nodesrow + num_bias):\n",
    "                        weights[str(i+1) + str(j) + \"0\"] = random.uniform(0,0.5)\n",
    "                    \n",
    "            return weights\n",
    "         \n",
    "        #_____Feedforward_____\n",
    "        def evaluate(values, weights, current_layer=0):\n",
    "            \"\"\"\n",
    "            (!This should be as simple as possible, as it needs to run potentially millions of times.)\n",
    "            Make a prediction (yhat), given values and weights. \n",
    "            \"\"\"\n",
    "            new_nodes = []\n",
    "            \n",
    "            if current_layer != self.depth: # must pass through entire network\n",
    "                \n",
    "                for outnode in range(self.nodesrow):\n",
    "                    summ = 0 \n",
    "                    for innode in range(len(values)-1):\n",
    "                        summ += weights[str(current_layer)+str(innode)+str(outnode)] * values[innode] # dot product\n",
    "                    new_nodes.append(self.hnaf(summ)) # apply activation function\n",
    "                    \n",
    "                for biasnum in range(self.num_bias): # take care of bias terms (act as nodes for next layer)\n",
    "                    new_nodes.append(weights[str(current_layer)+\"b\"+str(biasnum)])\n",
    "\n",
    "                return evaluate(values=new_nodes, weights=weights, current_layer=current_layer+1) # recursion\n",
    "            \n",
    "            else: # output layer\n",
    "                summ = 0 #E\n",
    "                for innode in range(len(values)):\n",
    "                    summ += weights[str(current_layer)+str(innode)+\"0\"] * values[innode] # dot product\n",
    "                return summ # identity activation function for output\n",
    "                 \n",
    "        def find_loss(xdata, ydata, weights):\n",
    "            \"\"\"\n",
    "            Calculate loss of neural network with current weights. \n",
    "            \"\"\"\n",
    "            yhats = []\n",
    "\n",
    "            for datum in range(len(xdata)): # for every individual in the data: \n",
    "                yhats.append(evaluate(xdata[datum], weights)) # compute loss\n",
    "        \n",
    "            return lossfunc(ydata, yhats)\n",
    "\n",
    "        #_____Back Propogation_____\n",
    "        def backprop(xdata, ydata, weights, max_attempts, lr):\n",
    "            \"\"\"\n",
    "            Perform gradient descent to calculate the new value of each weight in the neural network. \n",
    "            \"\"\"\n",
    "            h = 0.000000000001 # lim x->0\n",
    "            rec = []\n",
    "                \n",
    "            for trial in range(max_attempts): # user-specified number of trials\n",
    "                rec.append({})\n",
    "                print(f\"Trial {trial} loss: \", find_loss(xdata, ydata, weights))\n",
    "                \n",
    "                for weight in list(weights.keys()): \n",
    "                    loss = find_loss(xdata, ydata, weights)\n",
    "                    hweights = weights.copy()\n",
    "                    hweights.update({weight:weights[weight]+h})\n",
    "                    hloss = find_loss(xdata, ydata, hweights)\n",
    "                    newweight = weights[weight] - lr * (hloss - loss)/h\n",
    "                    weights[weight] = newweight  # gradient descent is complete -- weight is adjusted\n",
    "                    rec[trial][weight] = newweight # record the weight to examine training progress later\n",
    "                        \n",
    "            # display final loss\n",
    "            print(\"final loss: \", find_loss(xdata, ydata, weights))\n",
    "            return weights, rec\n",
    "            \n",
    "        self.weights = generate_weights(self.xdata, self.depth, self.nodesrow, self.num_bias)\n",
    "        self.weights, display.displaydata = backprop(self.inputs, ydata, self.weights, self.max_attempts, self.lr) # recurse\n",
    "        print(\"Fit complete.\")\n",
    "        \n",
    "    #_____Predict_____  \n",
    "    def predict(self, xdata):\n",
    "        \"\"\"\n",
    "        Give predictions for xdata using weights generated in .modfit. WARNING: Predict will not function without first creating weights in .modfit. \n",
    "        \"\"\"\n",
    "\n",
    "        self.inputs = xdata\n",
    "\n",
    "        def evaluate(values, weights, current_layer=0):\n",
    "            \"\"\"\n",
    "            (This should be as simple as possible, as it needs to run potentially millions of times.)\n",
    "            Make a prediction (yhat), given values and weights. \n",
    "            \"\"\"\n",
    "            new_nodes = []\n",
    "            \n",
    "            if current_layer != self.depth: # not at end yet\n",
    "                \n",
    "                for outnode in range(self.nodesrow):\n",
    "                    summ = 0 #NE\n",
    "                    for innode in range(len(values)-1):\n",
    "                        summ += weights[str(current_layer)+str(innode)+str(outnode)] * values[innode] # dot product\n",
    "                    new_nodes.append(self.hnaf(summ)) # apply activation function\n",
    "                    \n",
    "                for biasnum in range(self.num_bias): # take care of bias terms (act as nodes for next layer)\n",
    "                    new_nodes.append(weights[str(current_layer)+\"b\"+str(biasnum)])\n",
    "\n",
    "                return evaluate(values=new_nodes, weights=weights, current_layer=current_layer+1) # recursion\n",
    "            \n",
    "            else: # output layer\n",
    "                summ = 0 #E\n",
    "                for innode in range(len(values)):\n",
    "                    summ += weights[str(current_layer)+str(innode)+\"0\"] * values[innode] # dot product\n",
    "                return summ # identity activation function for output\n",
    "                 \n",
    "        preds = [] # to contain y values\n",
    "        for datum in self.inputs: # for every individual in the data: \n",
    "            preds.append(evaluate(self.inputs[datum], self.weights))\n",
    "\n",
    "        return preds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on Toy Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0 loss:  0.30427681386540206\n",
      "Trial 1 loss:  0.1724974931809114\n",
      "Trial 2 loss:  0.1619485125545938\n",
      "Trial 3 loss:  0.15892762489884585\n",
      "Trial 4 loss:  0.15624761475513005\n",
      "Trial 5 loss:  0.15315210909071242\n",
      "Trial 6 loss:  0.14945358077854373\n",
      "Trial 7 loss:  0.14499128677695988\n",
      "Trial 8 loss:  0.13957132494222363\n",
      "Trial 9 loss:  0.13312687163224932\n",
      "Trial 10 loss:  0.12535658763968158\n",
      "Trial 11 loss:  0.11604929023041943\n",
      "Trial 12 loss:  0.10508061332467997\n",
      "Trial 13 loss:  0.09248279545333898\n",
      "Trial 14 loss:  0.07855138751382103\n",
      "Trial 15 loss:  0.06390791976353317\n",
      "Trial 16 loss:  0.04947090031056961\n",
      "Trial 17 loss:  0.03626518560332445\n",
      "Trial 18 loss:  0.025139738465759547\n",
      "Trial 19 loss:  0.016520817589421356\n",
      "Trial 20 loss:  0.010358055403498302\n",
      "Trial 21 loss:  0.006256777658082316\n",
      "Trial 22 loss:  0.0036855795163487893\n",
      "Trial 23 loss:  0.0021460205555331718\n",
      "Trial 24 loss:  0.0012978878750849695\n",
      "Trial 25 loss:  0.0008563308638112676\n",
      "Trial 26 loss:  0.0005853594493263212\n",
      "Trial 27 loss:  0.0004223329571571639\n",
      "Trial 28 loss:  0.000306917559842684\n",
      "Trial 29 loss:  0.00023177543296724383\n",
      "Trial 30 loss:  0.00017588172041462712\n",
      "Trial 31 loss:  0.0001383178043685048\n",
      "Trial 32 loss:  0.0001081179998577925\n",
      "Trial 33 loss:  8.486262774072315e-05\n",
      "Trial 34 loss:  6.676742850215771e-05\n",
      "Trial 35 loss:  5.259937180935793e-05\n",
      "Trial 36 loss:  4.146498960508087e-05\n",
      "Trial 37 loss:  3.2696295338591866e-05\n",
      "Trial 38 loss:  2.5783197465542866e-05\n",
      "Trial 39 loss:  2.0330272776038543e-05\n",
      "Trial 40 loss:  1.6028418734464945e-05\n",
      "Trial 41 loss:  1.2634780611422945e-05\n",
      "Trial 42 loss:  9.958002549718373e-06\n",
      "Trial 43 loss:  7.847119777690729e-06\n",
      "Trial 44 loss:  6.182753513807996e-06\n",
      "Trial 45 loss:  4.870746836702237e-06\n",
      "Trial 46 loss:  3.836652790524072e-06\n",
      "Trial 47 loss:  3.021753355105465e-06\n",
      "Trial 48 loss:  2.3796993176744612e-06\n",
      "Trial 49 loss:  1.873900616386594e-06\n",
      "Trial 50 loss:  1.4754889405230668e-06\n",
      "Trial 51 loss:  1.1617051842914556e-06\n",
      "Trial 52 loss:  9.1458980239679e-07\n",
      "Trial 53 loss:  7.2000359665871e-07\n",
      "Trial 54 loss:  5.667944521277762e-07\n",
      "Trial 55 loss:  4.461622755393723e-07\n",
      "Trial 56 loss:  3.511935051447181e-07\n",
      "Trial 57 loss:  2.7642945125180586e-07\n",
      "Trial 58 loss:  2.1757453702775264e-07\n",
      "Trial 59 loss:  1.7124720406343197e-07\n",
      "Trial 60 loss:  1.3478088911606695e-07\n",
      "Trial 61 loss:  1.0607784204463046e-07\n",
      "Trial 62 loss:  8.348597526220222e-08\n",
      "Trial 63 loss:  6.570456849055524e-08\n",
      "Trial 64 loss:  5.170923061958458e-08\n",
      "Trial 65 loss:  4.06946373222298e-08\n",
      "Trial 66 loss:  3.202588454609604e-08\n",
      "Trial 67 loss:  2.5203358993688648e-08\n",
      "Trial 68 loss:  1.9834206571289878e-08\n",
      "Trial 69 loss:  1.5608756260141215e-08\n",
      "Trial 70 loss:  1.2283437188089094e-08\n",
      "Trial 71 loss:  9.666472650779652e-09\n",
      "Trial 72 loss:  7.607012648284047e-09\n",
      "Trial 73 loss:  5.9862844014327276e-09\n",
      "Trial 74 loss:  4.710864166219887e-09\n",
      "Trial 75 loss:  3.7071716933724435e-09\n",
      "Trial 76 loss:  2.917297513617429e-09\n",
      "Trial 77 loss:  2.2957121892545007e-09\n",
      "Trial 78 loss:  1.8065633242467634e-09\n",
      "Trial 79 loss:  1.4216354269665006e-09\n",
      "Trial 80 loss:  1.1187151709858453e-09\n",
      "Trial 81 loss:  8.803496231556245e-10\n",
      "Trial 82 loss:  6.927640399717955e-10\n",
      "Trial 83 loss:  5.451490026940774e-10\n",
      "Trial 84 loss:  4.2898769539735927e-10\n",
      "Trial 85 loss:  3.3758017133368493e-10\n",
      "Trial 86 loss:  2.656483554344202e-10\n",
      "Trial 87 loss:  2.0904446319480107e-10\n",
      "Trial 88 loss:  1.6450030662095204e-10\n",
      "Trial 89 loss:  1.2944826796408026e-10\n",
      "Trial 90 loss:  1.0186474494260724e-10\n",
      "Trial 91 loss:  8.015944462182209e-11\n",
      "Trial 92 loss:  6.307858003536808e-11\n",
      "Trial 93 loss:  4.9637763097679655e-11\n",
      "Trial 94 loss:  3.9060792449583e-11\n",
      "Trial 95 loss:  3.073758806693287e-11\n",
      "Trial 96 loss:  2.4187994744997997e-11\n",
      "Trial 97 loss:  1.9033851592720615e-11\n",
      "Trial 98 loss:  1.4978041517531518e-11\n",
      "Trial 99 loss:  1.1786420016671829e-11\n",
      "final loss:  9.274885842065075e-12\n",
      "Fit complete.\n",
      "CPU times: user 267 ms, sys: 5.51 ms, total: 273 ms\n",
      "Wall time: 292 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9999999867800722,\n",
       " 4.1841453630731e-06,\n",
       " -4.42631858121878e-06,\n",
       " 0.9999999867800722]"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# run\n",
    "net = my_neural_net(depth=2, nodesrow=2, num_bias=2)\n",
    "net.modfit(xdata = xdata, \n",
    "           ydata = ydata, \n",
    "           max_attempts=100, \n",
    "           lr=0.4)\n",
    "preds = net.predict(xdata)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'000': -0.1923525502294713,\n",
       " '001': -0.31257258708577124,\n",
       " '010': -0.005745792500823634,\n",
       " '011': 0.11138078664047608,\n",
       " '020': 0.5225472435267449,\n",
       " '021': 0.6443961863562039,\n",
       " '030': 0.19233785109979717,\n",
       " '031': 0.4239671571206148,\n",
       " '0b0': 0.1502895282804971,\n",
       " '0b1': 0.8071135467672128,\n",
       " '100': 0.7083411188972699,\n",
       " '101': 0.15166402596479603,\n",
       " '110': 0.5587482215985207,\n",
       " '111': 0.5766042740733474,\n",
       " '120': 0.3494086881071112,\n",
       " '121': 0.39112041195540204,\n",
       " '130': 0.2994648540129222,\n",
       " '131': 0.29323250136952006,\n",
       " '1b0': 0.7577493985773983,\n",
       " '1b1': 0.7854560265544022,\n",
       " '200': 0.7108496391124751,\n",
       " '210': 0.39457285510505336,\n",
       " '220': -0.09255433035420506,\n",
       " '230': -0.07707137319040144}"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at weights. are any particularly massive? \n",
    "net.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [0, 0, 1, 1, 0],\n",
       " 1: [1, 0, 0, 1, 0],\n",
       " 2: [0, 1, 0, 0, 1],\n",
       " 3: [0, 0, 1, 1, 1]}"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset, for reference\n",
    "xdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9999999867800722,\n",
       " 4.1841453630731e-06,\n",
       " -4.42631858121878e-06,\n",
       " 0.9999999867800722]"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predictions\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 1, 1: 0, 2: 0, 3: 1}"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ydata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.159225949982524e-06"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean absolute error (MAE) between dataset and predictions\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "mae(list(ydata.values()), preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b01bdd6e52ac1524ff49a01b98266f456fa102714d338122dc89e1020ace209a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
